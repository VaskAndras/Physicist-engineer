## Definition of a Matrix
- A matrix $A$ is a rectangular array of numbers arranged in **rows** and **columns**.  
  Example:
  $$
  A = \begin{bmatrix}
  1 & 4 \\
  2 & 6 \\
  3 & 5
  \end{bmatrix}
  $$
  - $m = 3$ rows, $n = 2$ columns → $A$ is a $3 \times 2$ matrix.

>[!note] (Matrix Dimensions)  
>For a matrix $A$ with $m$ rows and $n$ columns, we write: $A \in \mathbb{R}^{m \times n}$.

---

## Matrix-Vector Multiplication
- Consider a vector $x = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \in \mathbb{R}^2$.  
- Multiplying $A$ by $x$:  
  $$
  A x = \begin{bmatrix} 1 & 4 \\ 2 & 6 \\ 3 & 5 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} 
      = x_1 \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} + x_2 \begin{bmatrix} 4 \\ 6 \\ 5 \end{bmatrix}
  $$
- Key idea: **Matrix-vector multiplication is a linear combination of the columns of $A$**, weighted by the components of $x$.  

**Magyar röviden:**  
- $Ax$ = lineáris kombináció az $A$ oszlopvektoraiból, súlyozva $x$ komponenseivel.

---

## Components as Dot Products
- Another viewpoint: multiply **row by row**:  
  - If $A$ has rows $r_1, r_2, r_3$ then  
    $$
    (Ax)_i = r_i \cdot x
    $$
  - Each component of $Ax$ is the **dot product of a row of $A$ with the vector $x$**.

>[!note] (Dot Product in Matrix Multiplication)  
>Each entry of $Ax$ can be written as a dot product of a row of $A$ with the vector $x$.

---

## Linear Combination Interpretation
- Let vectors $u, v, w \in \mathbb{R}^3$.  
- Any vector $b$ as a combination:  
  $$
  b = x_1 u + x_2 v + x_3 w
  $$
- Rewrite as matrix multiplication:  
  $$
  A = [u \; v \; w], \quad x = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}, \quad Ax = b
  $$
- **Insight:** Previously, numbers $x_i$ multiplied the vectors. Now $A$ multiplies $x$, producing $b$ as a **linear combination of columns**.

---

## Example: Difference Matrix
- Input vector $x = \begin{bmatrix} 1 \\ 4 \\ 9 \end{bmatrix}$ (squares).  
- Define $A$ such that $b = Ax$ contains differences between consecutive elements:  
  $$
  b_1 = x_1 - 0 = 1, \quad b_2 = x_2 - x_1 = 3, \quad b_3 = x_3 - x_2 = 5
  $$
- **Pattern**: the matrix $A$ computes all differences at once: $[1,3,5]$.

---

## Important Note on Multiplication Methods
- Two equivalent ways to compute $Ax$:  
  1. **Row-wise (dot product):** each entry of $Ax$ is $r_i \cdot x$.  
  2. **Column-wise (linear combination):** $Ax = x_1 c_1 + x_2 c_2 + \dots + x_n c_n$, where $c_i$ are columns of $A$.  

>[!note] (Linear Combination Perspective)  
>Column-based viewpoint is **central in linear algebra**: the output $Ax$ is always a combination of the columns of $A$.

- For numeric computations, row-wise is often easier; for algebraic reasoning, column-wise provides insight into **linear dependence, span, and basis**.

---

## Summary
- Matrices represent **linear transformations**: $A$ acts on vector $x$, producing output $b = Ax$.  
- Two perspectives on $Ax$:  
  1. Row-wise (dot product)  
  2. Column-wise (linear combination)  
- Difference matrices illustrate practical use: compute differences, derivatives, or apply discrete operators.  

**Magyar röviden:**  
- Mátrix = lineáris transzformáció, $Ax$ = kimenet.  
- Oszlopok lineáris kombinációja, sorokkal dot product.

# Linear Equations 

## Change of Viewpoint
- Previously: we knew the **vector of coefficients** $x = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}$ and computed the output $b$ via:
  $$
  b = A x = x_1 u + x_2 v + x_3 w
  $$
  - Here, $u, v, w$ are column vectors of the matrix $A$.

- New viewpoint: **$b$ is known, $x$ is unknown**.  
  - Question: *Which combination of $u, v, w$ produces a given vector $b$?*  
  - This is the **inverse problem** of linear algebra: find the input $x$ that gives the desired output $b$.

>[!note] (Inverse Problem)  
>Given $b$ and $A$, solve $Ax = b$ to find the vector $x$ of coefficients.

---

## Connection to Linear Systems
- Let $A$ be a $3 \times 3$ matrix and $b \in \mathbb{R}^3$. Then solving $Ax = b$ is equivalent to solving a system of linear equations:
  $$
  \begin{cases}
  r_1 \cdot x = b_1 \\
  r_2 \cdot x = b_2 \\
  r_3 \cdot x = b_3
  \end{cases}
  $$
  where $r_i$ are the **rows of $A$**.

- Example: Triangular matrix $A$:
  $$
  A = \begin{bmatrix}
  1 & 0 & 0 \\
  1 & 1 & 0 \\
  1 & 1 & 1
  \end{bmatrix}, \quad
  x = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}, \quad
  b = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix}
  $$
- Solving top to bottom:
  1. First equation: $x_1 = b_1$  
  2. Second equation: $x_1 + x_2 = b_2 \implies x_2 = b_2 - x_1 = b_2 - b_1$  
  3. Third equation: $x_1 + x_2 + x_3 = b_3 \implies x_3 = b_3 - x_1 - x_2 = b_3 - b_1 - (b_2 - b_1) = b_3 - b_2$

- Thus the solution vector is:
  $$
  x = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} b_1 \\ b_2 - b_1 \\ b_3 - b_2 \end{bmatrix}
  $$

---

## Example Solutions
- Case 1: $b = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} \implies x = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}$.  
  - Interpretation: if the output is zero, the input must be zero for this matrix $A$.  
  - This property is **not true for all matrices**.

- Case 2: $b = \begin{bmatrix} 1 \\ 3 \\ 5 \end{bmatrix} \implies x = \begin{bmatrix} 1 \\ 2 \\ 2 \end{bmatrix}$ (following the same procedure).  
  - Here we recover the input $x$ that produces $b$ using $A$.

>[!note] (Invertible Matrix)  
>A matrix $A$ is **invertible** if, for every $b$, there exists a unique $x$ such that $Ax = b$.  
>In that case, we write: $x = A^{-1} b$.

---

## Key Observations
1. **Triangular matrices** allow simple sequential solving (top to bottom).  
2. **Invertibility** guarantees a unique solution.  
3. **Non-invertible matrices** may have no solution or infinitely many solutions.  
4. The equation $Ax = b$ is fundamental in linear algebra: it connects **matrix multiplication** to **linear combinations of vectors**.

**Magyar röviden:**  
- $Ax = b$ → lineáris kombináció visszafejtése, invertibilitás esetén $x = A^{-1}b$.
# The Inverse Matrix 

## Recap of the Solution
- From the previous section, solving $Ax = b$ gave:
  $$
  x = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = 
      \begin{bmatrix} b_1 \\ b_1 + b_2 \\ b_1 + b_2 + b_3 \end{bmatrix}
  $$
- Observation: **differences of $x$ are the $b$'s, sums of $b$'s are the $x$'s**.  
  - Example: $b = \begin{bmatrix} 1 \\ 3 \\ 5 \end{bmatrix}$ and $x = \begin{bmatrix} 1 \\ 4 \\ 9 \end{bmatrix}$ (squares).  

>[!note] (Inverse Matrix Definition)  
>The **sum matrix** in this formula is the **inverse** $A^{-1}$ of the difference matrix $A$.  
>It transforms the differences (b) back into the original values (x).

---

## Example of the Inverse Matrix
- Let $x = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$, then the differences are:
  $$
  b = Ax = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}
  $$
- Using the inverse:
  $$
  x = A^{-1} b
  $$
- **Key Facts** from Equation (7):
  1. For **every vector $b$**, there exists exactly **one solution** $x$ such that $Ax = b$.  
  2. The **inverse matrix** $A^{-1}$ produces the solution: $x = A^{-1} b$.

>[!note] (Key Fact)  
>If $A$ is invertible, **every output $b$ has a unique input $x$**, and $A^{-1}$ recovers it.

---

## Connection to Calculus
- Replace the discrete vector $x$ with a function $x(t)$:  
  - Differences $Ax$ → derivative $dx/dt = b(t)$  
  - Sums $A^{-1} b$ → integral of $b(t)$
- Fundamental analogy: **integration is the inverse of differentiation**
  $$
  Ax = b \quad \longleftrightarrow \quad \frac{dx}{dt} = b(t), \quad 
  x = A^{-1}b \quad \longleftrightarrow \quad x(t) = \int b(t) dt
  $$

- Example with squares:
  - Sequence of squares: $0, 1, 4, 9$  
  - Differences: $1, 3, 5$  
  - Derivative analogy: $x(t) = t^2 \implies dx/dt = 2t$  
  - Note: **forward differences** give $2t - 1$, not exactly $2t$:
    $$
    x(t) - x(t-1) = t^2 - (t-1)^2 = 2t - 1
    $$
- **Better approximation:** centered difference:
  $$
  \frac{x(t+1) - x(t-1)}{2} = \frac{(t+1)^2 - (t-1)^2}{2} = 2t
  $$
  - Produces the exact derivative for $x(t) = t^2$.

>[!note] (Centered Difference)  
>Using $x(t+1) - x(t-1)$ divided by $2$ yields the **exact derivative** in discrete steps.  
>Difference matrices are powerful tools; **centered differences** are often the most accurate.

---

## Summary of the Inverse Matrix Concept
1. **Difference Matrix $A$:** transforms $x$ into differences $b = Ax$.  
2. **Inverse $A^{-1}$:** transforms differences $b$ back into original $x$.  
3. This idea extends naturally to **functions and calculus**, connecting **discrete differences to derivatives**.  
4. Not all matrices are invertible — some outputs $b$ may not correspond to a unique input $x$.  

**Magyar röviden:**  
- $A^{-1}$ = “sum matrix”, visszaadja az $x$-et a különbségekből $b$-ből.  
- Kapcsolat a kalkulussal: $\Delta x \sim dx/dt$, $\sum b \sim \int b(t) dt$.
# Cyclic Differences and Linear Dependence

## Introduction
- In the previous section, we solved $Ax = b$ with a triangular (difference) matrix $A$.  
- Now we change the third vector $w$ to a new vector $w^*$ and form a **cyclic difference matrix** $C$:
  $$
  C = 
  \begin{bmatrix}
  1 & -1 & -1 \\ 
  -1 & 1 & 0 \\ 
  0 & -1 & 1
  \end{bmatrix} \quad 
  \text{so that } Cx = b
  $$
- **Cyclic differences:** $x_1 - x_3$, $x_2 - x_1$, $x_3 - x_2$.

>[!note] (Cyclic Difference Matrix)  
>Unlike the triangular difference matrix $A$, $C$ is **not triangular**, so solving $Cx = b$ is more complex.  

---

## Impossibility of a Solution
- **Case 1:** $Cx = 0$ has infinitely many solutions:
  $$
  x_1 - x_3 = 0, \quad x_2 - x_1 = 0, \quad x_3 - x_2 = 0
  $$
  - All **constant vectors** $x = (c, c, c)$ satisfy this.  
  - The **undetermined constant $c$** is analogous to the $+C$ in integrals.

- **Case 2:** $Cx = b$ with $b = (1, 3, 5)$  
  - Left sides always sum to zero: $(x_1 - x_3) + (x_2 - x_1) + (x_3 - x_2) = 0$  
  - Right sides sum to 9 → no solution exists.  

>[!note] (Geometric View)  
>All combinations $x_1 u + x_2 v + x_3 w^*$ lie on the **plane $b_1 + b_2 + b_3 = 0$**.  
>If $b$ does not satisfy this, **no solution exists**.

---

## Linear Dependence and Independence
- **Vectors in columns:**
  - $A = [u \ v \ w]$ → independent, fill 3D space.
  - $C = [u \ v \ w^*]$ → dependent, lie on a plane.
  
- **Key definitions:**
  1. **Independence:** a vector (e.g., $w$) is **not in the plane** of $u$ and $v$.
  2. **Dependence:** a vector (e.g., $w^*$) **lies in the plane** of $u$ and $v$.  
     $$
     u + v + w^* = 0 \implies w^* = -u - v
     $$

- **Implications for matrices:**
  - Independent columns → $Ax = 0$ has **one solution**, $A$ invertible.
  - Dependent columns → $Cx = 0$ has **infinitely many solutions**, $C$ singular.

>[!note] (Visual Intuition)  
>In 3D:  
>- $u, v, w$ span the whole space → unique solutions.  
>- $u, v, w^*$ span a plane → multiple or no solutions depending on $b$.

---

## Summary of Cyclic Differences
1. Triangular difference matrix → unique solutions via $A^{-1}$.
2. Cyclic difference matrix → not always solvable; solutions exist **only if** $b_1 + b_2 + b_3 = 0$.  
3. Independence of vectors = columns span full space → invertible.  
4. Dependence of vectors = columns span a plane → singular.

**Magyar röviden:**  
- $w^*$ lineárisan függ $u$-tól és $v$-tól → $Cx = b$ lehetetlen, ha $b$ nem a síkban van.  
- Függőség vs. függetlenség a mátrix invertibilitását döntő tényező.