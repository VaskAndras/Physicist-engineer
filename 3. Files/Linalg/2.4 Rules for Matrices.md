# 2.4 Rules for Matrix Operations

## Introduction

Matrix operations form the foundation of linear algebra.  
They follow **specific structural rules** — not every pair of matrices can be multiplied,  
and the order of multiplication **matters**.  
This section introduces how and when matrices can be multiplied,  
and explains **four fundamental ways** to think about the product $AB$.

---

## 1. The Shape Rule – When Is $AB$ Defined?

If $A$ has **n columns**, then $B$ must have **n rows**.  
That’s the **dimensional compatibility condition** for multiplication.

Formally:
$$
A_{m \times n} \cdot B_{n \times p} = C_{m \times p}
$$

This means:
- The **inner dimensions** (n and n) must match.
- The **outer dimensions** (m and p) determine the size of the result.

**Example:**
- If $A$ is 3×2, it can multiply:
  - a 2×1 column vector,
  - a 2×2 square matrix,
  - or a 2×20 wide matrix.

Magyarul:  
A szorzás csak akkor értelmezett, ha az első mátrix oszlopainak száma megegyezik a második mátrix sorainak számával.  

---

## 2. The Fundamental Law of Matrix Multiplication

> **Associative Law:**  
> $$(AB)C = A(BC)$$

This is called the **Fundamental Law**, and it ensures that matrix multiplication  
is **associative** (the parentheses can move) but **not commutative** (the order matters).

Magyarul:  
A zárójelek mozgathatók, de a sorrend nem cserélhető fel, vagyis általában $AB \neq BA$.

---

## 3. The First and Most Common Definition – Row by Column Rule

When you multiply two matrices,  
**each entry** of the product $AB$ is found by taking the **dot product**  
of a row of $A$ with a column of $B$.

### Definition
$$
(AB)_{ij} = \text{(row i of A)} \cdot \text{(column j of B)}
$$

Each entry is a **sum of elementwise products**.

**Example:**
If $A$ is 4×5 and $B$ is 5×6, then $AB$ is 4×6.

To find the element in the 2nd row and 3rd column:
$$
(AB)_{23} = a_{21}b_{13} + a_{22}b_{23} + a_{23}b_{33} + a_{24}b_{43} + a_{25}b_{53}
$$

Magyarul:  
Az eredmény minden eleme egy **sor és oszlop skaláris szorzata**.

---

## 4. Example 1 – Square Matrices

When both $A$ and $B$ are **n×n square matrices**,  
their product $AB$ is also n×n.

Each of the $n^2$ entries in $AB$ is a dot product,  
and each dot product requires **n multiplications**.  
Therefore, computing $AB$ requires **$n^3$ multiplications**.

| n | Number of multiplications |
|---|----------------------------|
| 2 | 8                         |
| 100 | 1,000,000               |

For decades, mathematicians believed this $n^3$ rule was unavoidable.  
Then **Volker Strassen** discovered a clever trick —  
you can multiply two 2×2 matrices using only **7 multiplications**  
(by reusing intermediate sums cleverly).  

When this idea is applied recursively to larger matrices,  
it reduces the number of multiplications to **$n^{2.376}$**.  

However, these “fast matrix multiplication” algorithms are  
**computationally awkward** — they’re hard to implement and use more memory.  
Thus, scientific computing still uses the standard $n^3$ approach.

Magyarul:  
A Strassen-módszer elvileg gyorsabb, de a gyakorlatban túl bonyolult,  
ezért maradunk a klasszikus szorzásnál.

---

## 5. Example 2 – Row Vector × Column Vector and Vice Versa

### Inner Product (Row × Column)
If $A$ is 1×3 and $B$ is 3×1:

$$
AB = [a_1, a_2, a_3]
\begin{bmatrix}
b_1 \\ b_2 \\ b_3
\end{bmatrix}
= a_1b_1 + a_2b_2 + a_3b_3
$$

The result is **1×1**, a single number —  
the **dot product**, also called the **inner product**.

Magyarul:  
Sor × oszlop = skaláris szorzat, egyetlen szám.

---

### Outer Product (Column × Row)
If we reverse the order — $B$ is 3×1 and $A$ is 1×3:

$$
BA =
\begin{bmatrix}
b_1 \\ b_2 \\ b_3
\end{bmatrix}
[a_1, a_2, a_3]
=
\begin{bmatrix}
b_1a_1 & b_1a_2 & b_1a_3 \\
b_2a_1 & b_2a_2 & b_2a_3 \\
b_3a_1 & b_3a_2 & b_3a_3
\end{bmatrix}
$$

Now the result is **3×3**, a full matrix.  
Each entry represents one pairwise product of components.

This is called the **outer product**.  
It constructs a matrix from two vectors.

Magyarul:  
Oszlop × sor = mátrix, amely az összes lehetséges szorzatot tartalmazza.

---

## 6. Summary (Hungarian)

- A szorzás csak akkor értelmezett, ha az első mátrix oszlopainak száma  
  megegyezik a második mátrix sorainak számával.  
- Minden elem a megfelelő sor és oszlop **skaláris szorzata**.  
- A művelet **asszociatív**, de **nem kommutatív**.  
- Négyzetes mátrixok szorzása $n^3$ műveletet igényel.  
- **Inner product** → szám, **outer product** → mátrix.
# 2.4 Rules for Matrix Operations (Continuation)

## The Second and Third Ways: Rows and Columns

Matrix multiplication can be understood in **four different but equivalent** ways.  
The first one — the **dot product rule** — we already covered.  
Now let’s carefully look at the **second** and **third** perspectives,  
which give a much deeper geometric and structural understanding of what happens  
when two matrices are multiplied.

---

## 1. The Column Picture — “A multiplies every column of B”

Let’s recall that if $A$ is $m \times n$ and $B$ is $n \times p$,  
then the result $AB$ is an $m \times p$ matrix.

Now focus on **one column** of $B$, say column $j$, written as $b_j$ (a vector).

Then:
$$
A B = [A b_1 \; A b_2 \; A b_3 \; \dots \; A b_p]
$$

That is, **each column of $AB$** is the result of multiplying $A$ by the corresponding **column of $B$**.

Magyarul:  
A $B$ mátrix minden oszlopát megszorozzuk az $A$ mátrixszal,  
és az így kapott vektorokból áll össze az $AB$ mátrix.

---

### Geometric meaning

- $A$ “acts on” each column of $B$.
- Each column of $AB$ is a **linear combination of the columns of $A$**.  
  The coefficients of this combination are taken from the column of $B$ we’re currently multiplying.

Formally:
$$
AB = A[b_1 \; b_2 \; \dots \; b_p] = [A b_1 \; A b_2 \; \dots \; A b_p]
$$

Thus, every new column in $AB$ lies in the **column space of $A$**.  
This means matrix multiplication by $A$ transforms or mixes the columns of $B$  
within the subspace spanned by $A$’s own columns.

**Example (conceptual):**

If $A$ represents a linear transformation (e.g., stretching, rotating),  
then multiplying $A$ by $B$ means we apply that transformation  
to each column vector of $B$.

---

## 2. The Row Picture — “Each row of A multiplies the whole B”

Now we turn the idea upside down.

Instead of thinking in columns,  
we can think in terms of **rows**.

Every **row** of $A$ multiplies the **entire matrix $B$**  
and produces a **row** of $AB$.

$$
\text{Row}_i(AB) = \text{Row}_i(A) \cdot B
$$

Magyarul:  
Minden sor az $A$ mátrixból megszorozza az egész $B$ mátrixot,  
és így megkapjuk az $AB$ mátrix megfelelő sorát.

This means that each **row of $AB$** is a **linear combination of the rows of $B$**,  
with the coefficients coming from the row of $A$ that’s doing the multiplication.

---

### Example

Let’s say:

$$
A =
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix},
\quad
B =
\begin{bmatrix}
7 & 8 \\
9 & 10 \\
11 & 12
\end{bmatrix}
$$

Then:
- Row 1 of $A$ (which is $[1 \; 2 \; 3]$) multiplies the whole $B$
  $$
  [1 \; 2 \; 3] B = [1(7) + 2(9) + 3(11), \; 1(8) + 2(10) + 3(12)] = [58, 64]
  $$
- Row 2 of $A$ (which is $[4 \; 5 \; 6]$) multiplies $B$:
  $$
  [4 \; 5 \; 6] B = [4(7) + 5(9) + 6(11), \; 4(8) + 5(10) + 6(12)] = [139, 154]
  $$

So:
$$
AB =
\begin{bmatrix}
58 & 64 \\
139 & 154
\end{bmatrix}
$$

Each **row of the result** came from multiplying **one row of A with the full B**.

---

## 3. The Fourth Way — “Columns Multiply Rows”

Now comes the **most magical** and least intuitive form of multiplication.

Instead of seeing $A$ acting on $B$,  
we decompose the whole operation into **column–row products**.

Each **column of A** multiplies the corresponding **row of B**,  
and then we **add all those resulting matrices** together.

Formally:
$$
AB = \sum_{k=1}^{n} (\text{column } k \text{ of } A)(\text{row } k \text{ of } B)
$$

---

### Breaking it down

1. Take the **first column** of $A$, call it $a_1$,  
   and the **first row** of $B$, call it $b_1^T$.  
   Multiply them: $a_1 b_1^T$ → this gives an $m \times p$ matrix.
2. Do the same for all columns/rows ($a_2 b_2^T$, $a_3 b_3^T$, …).
3. Add them all up:
   $$
   AB = a_1 b_1^T + a_2 b_2^T + \dots + a_n b_n^T
   $$

This is called the **column–row decomposition** of matrix multiplication.

---

### Example with 2×2 matrices

Let:
$$
A =
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix},
\quad
B =
\begin{bmatrix}
E & F \\
G & H
\end{bmatrix}
$$

Then:
$$
A B = 
a
\begin{bmatrix}
E & F \\
0 & 0
\end{bmatrix}
+
b
\begin{bmatrix}
G & H \\
0 & 0
\end{bmatrix}
+
c
\begin{bmatrix}
0 & 0 \\
E & F
\end{bmatrix}
+
d
\begin{bmatrix}
0 & 0 \\
G & H
\end{bmatrix}
$$

Or equivalently, using the “column times row” viewpoint:
$$
AB =
[a \; c]^T [E \; F] +
[b \; d]^T [G \; H]
$$

This gives:
$$
AB =
\begin{bmatrix}
aE + bG & aF + bH \\
cE + dG & cF + dH
\end{bmatrix}
$$

Which matches exactly the usual computation.  
So the fourth way is *just a different perspective*, but it yields the same result.

---

### Why this is important

- The **column–row form** helps understand how matrix multiplication builds up layer by layer.  
  Each outer product $a_k b_k^T$ adds a *rank-1 component* to the final result.
- In advanced linear algebra, this idea becomes essential for:
  - **low-rank approximations**,
  - **data compression**,
  - **singular value decomposition (SVD)**.

Magyarul:  
A negyedik módszer szerint minden oszlop és sor párosából egy új mátrix keletkezik,  
és ezek összege adja ki az eredményt.  
Ez a szemlélet megmutatja, hogyan épül fel a mátrix sok „kis” mátrix összegeként.

---

## 4. Laws for Matrix Operations

Let’s now summarize the **laws matrices do obey**,  
and contrast them with the one rule they **don’t**.

### Addition Laws

1. **Commutative:** $A + B = B + A$  
   (order doesn’t matter)
2. **Distributive (with scalar):** $c(A + B) = cA + cB$
3. **Associative:** $A + (B + C) = (A + B) + C$

These always hold.

---

### Multiplication Laws

1. **Distributive (left):** $A(B + C) = AB + AC$
2. **Distributive (right):** $(A + B)C = AC + BC$
3. **Associative:** $A(BC) = (AB)C$

But **not commutative**:  
In general,
$$
AB \neq BA
$$

Even if both products are defined, their shapes or values differ.

---

### Example showing noncommutativity

Let:
$$
A =
\begin{bmatrix}
1 & 2 \\
0 & 3
\end{bmatrix},
\quad
B =
\begin{bmatrix}
4 & 0 \\
1 & 2
\end{bmatrix}
$$

Then:
$$
AB =
\begin{bmatrix}
6 & 4 \\
3 & 6
\end{bmatrix},
\quad
BA =
\begin{bmatrix}
4 & 8 \\
1 & 8
\end{bmatrix}
$$

Clearly $AB \neq BA$.

Only special matrices (like the identity $I$ or scalar multiples $cI$)  
commute with every other matrix.

---

## 5. Matrix Powers and Inverses

For square matrices, you can define powers like numbers:

$$
A^p = A \cdot A \cdot A \cdots A \quad (p \text{ times})
$$

They follow the same exponent rules:
$$
A^p A^q = A^{p+q}, \quad (A^p)^q = A^{pq}
$$

And the **zero power** gives the identity:
$$
A^0 = I
$$

For an **inverse matrix**:
$$
A^{-1} \text{ is defined so that } A A^{-1} = A^{-1} A = I
$$

This only exists when $A$ is **invertible** —  
a key topic in the next section (2.5).

---

## 6. Block Matrices and Block Multiplication (Expanded)

When dealing with large matrices, it is often useful to **divide a matrix into smaller submatrices**, called **blocks**.  
Each block is itself a matrix, and we can perform operations **block by block**, as long as the dimensions match correctly.  
This approach helps simplify **matrix addition, multiplication, and elimination**, especially for computational purposes.

---

### 6.1 What is a block matrix?

A **block matrix** is a matrix partitioned into smaller matrices.  

For example, consider a $4 \times 6$ matrix:

$$
A =
\begin{bmatrix}
a_{11} & a_{12} & a_{13} & a_{14} & a_{15} & a_{16} \\
a_{21} & a_{22} & a_{23} & a_{24} & a_{25} & a_{26} \\
a_{31} & a_{32} & a_{33} & a_{34} & a_{35} & a_{36} \\
a_{41} & a_{42} & a_{43} & a_{44} & a_{45} & a_{46}
\end{bmatrix}
$$

We can split it into **2×2 blocks**:

$$
A =
\begin{bmatrix}
A_{11} & A_{12} & A_{13} \\
A_{21} & A_{22} & A_{23}
\end{bmatrix}
\quad \text{where each } A_{ij} \text{ is } 2 \times 2
$$

Here:
- $A_{11} = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix}$
- $A_{12} = \begin{bmatrix} a_{13} & a_{14} \\ a_{23} & a_{24} \end{bmatrix}$
- $A_{13} = \begin{bmatrix} a_{15} & a_{16} \\ a_{25} & a_{26} \end{bmatrix}$
- $A_{21}, A_{22}, A_{23}$ are defined similarly from rows 3–4.

**Key idea:**  
We treat each block as a "single element" when performing **block addition or multiplication**, just like numbers.

---

### 6.2 Block Addition

Two matrices of the same shape can be **added block by block**, as long as the corresponding blocks have the same size:

$$
A + B =
\begin{bmatrix}
A_{11} & A_{12} & A_{13} \\
A_{21} & A_{22} & A_{23}
\end{bmatrix}
+
\begin{bmatrix}
B_{11} & B_{12} & B_{13} \\
B_{21} & B_{22} & B_{23}
\end{bmatrix}
=
\begin{bmatrix}
A_{11}+B_{11} & A_{12}+B_{12} & A_{13}+B_{13} \\
A_{21}+B_{21} & A_{22}+B_{22} & A_{23}+B_{23}
\end{bmatrix}
$$

Magyarul:  
Minden blokkot a megfelelő blokkal összeadunk. Ez teljesen megegyezik a “számszerű” összeadással, csak most minden blokk egy **mátrix**, nem egy szám.

---

### 6.3 Block Multiplication

Block multiplication is slightly more subtle.  
We can multiply **blocks of A** by **blocks of B**, as long as the **inner dimensions match** (like normal matrix multiplication).

#### 6.3.1 General rule

If we partition $A$ horizontally and $B$ vertically so that **the number of columns of each block in A equals the number of rows in the corresponding block of B**, then:

$$
AB =
\begin{bmatrix}
A_{11} & A_{12} \\
A_{21} & A_{22}
\end{bmatrix}
\begin{bmatrix}
B_{11} & B_{12} \\
B_{21} & B_{22}
\end{bmatrix}
=
\begin{bmatrix}
A_{11}B_{11} + A_{12}B_{21} & A_{11}B_{12} + A_{12}B_{22} \\
A_{21}B_{11} + A_{22}B_{21} & A_{21}B_{12} + A_{22}B_{22}
\end{bmatrix}
$$

**Explanation:**  
- Each block in the result is obtained **exactly like normal multiplication**, but treating each block as a “number.”  
- This preserves **row-column rules**: sum of products across the inner dimension.

---

### 6.4 Why block multiplication is useful

1. **Simplifies computations**: Large matrices can be handled in chunks.  
2. **Facilitates elimination**: We can apply Gaussian elimination to **entire columns or rows at once**.  
3. **Connection to Schur complement**: When performing elimination on block matrices, the lower-right block is updated as  
   $$
   S = D - C A^{-1} B
   $$  
   This is a **generalization** of the scalar formula $d - cb/a$.

---

### 6.5 Example of block elimination

Suppose we have:

$$
A =
\begin{bmatrix}
1 & 2 & 3 & 4 \\
5 & 6 & 7 & 8 \\
9 & 10 & 11 & 12 \\
13 & 14 & 15 & 16
\end{bmatrix}
=
\begin{bmatrix}
A & B \\
C & D
\end{bmatrix}
$$

Where each block is $2 \times 2$.  

We want to eliminate $C$ using the pivot block $A$:

1. Compute $C A^{-1}$.
2. Multiply by $B$.
3. Subtract from $D$:
$$
D_{\text{new}} = D - C A^{-1} B
$$

This **clears the lower-left block** while updating the lower-right block, exactly like standard elimination for numbers.

**Key point:**  
- $A$ is the **pivot block**.  
- $C$ is eliminated, $D$ is updated.  
- This is the **Schur complement**, fundamental in block elimination and LU factorization for larger systems.

---

### 6.6 Summary of Block Operations

- **Blocks are just smaller matrices inside a larger matrix.**
- **Addition:** add corresponding blocks.
- **Multiplication:** multiply blocks like numbers, respecting inner dimensions.
- **Elimination:** a pivot block can eliminate an entire column of blocks below it.
- **Schur complement:** generalizes $d - cb/a$ to block matrices:
  $$
  S = D - C A^{-1} B
  $$

Block matrices allow us to **see structure, simplify computations, and generalize elimination**, making them a powerful tool for large linear systems.