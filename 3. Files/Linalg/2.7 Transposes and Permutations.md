# 2.7 Transposes and Permutations

> NOTE: This section explains how **transposes, inner/outer products, symmetric and orthogonal matrices, and permutation matrices** work.

---

## 1. Transpose Definition

- The **transpose** of a matrix $A$ is denoted by $A^T$.
- **Definition**: Flip the matrix across its main diagonal.
- If $A$ is $m \times n$, then $A^T$ is $n \times m$.
- Entry-wise:
$$
(A^T)_{ij} = A_{ji}
$$
- **Special cases**:
  - Transpose of a **lower triangular** matrix → upper triangular.
  - Transpose of the transpose: $$(A^T)^T = A$$

> NOTE: In MATLAB, `'` is used for transpose. Row vectors can be transposed into column vectors.

---

## 2. Transpose Rules

1. **Sum**:
$$
(A + B)^T = A^T + B^T
$$
2. **Product**:
$$
(AB)^T = B^T A^T
$$
> NOTE: Reverse order is crucial for products.

3. **Inverse**:
$$
(A^{-1})^T = (A^T)^{-1}
$$
- **Reason**: $(AB)^T = B^T A^T$ and $A A^{-1} = I$ ensures transpose of inverse works.

---

## 3. Inner and Outer Products

- **Inner (dot) product** of vectors $x$ and $y$:
$$
x \cdot y = x^T y
$$
- **Outer product**:
$$
xy^T
$$
- Dimensions:
  - $x^T y$: $(1 \times n)(n \times 1) = 1 \times 1$ (scalar)
  - $xy^T$: $(n \times 1)(1 \times n) = n \times n$ (matrix)
- **Applications**:
  - Mechanics: Work = $x^T f$
  - Circuits: Heat loss = $e^T y$
  - Economics: Income = $q^T p$

> NOTE: The inner product allows connection to transpose: 
$$
(Ax)^T y = x^T (A^T y)
$$

---

## 4. Symmetric Matrices

- **Definition**: A matrix $S$ is symmetric if
$$
S^T = S \quad \text{or equivalently} \quad S_{ij} = S_{ji}
$$
- **Properties**:
  - $A^T A$ is always symmetric.
  - Inverse of a symmetric matrix is also symmetric:
$$
(S^{-1})^T = (S^T)^{-1} = S^{-1}
$$
- Example:
$$
S = \begin{bmatrix} 5 & -2 \\ -2 & 1 \end{bmatrix}, \quad S^{-1} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}
$$

---

## 5. Orthogonal Matrices

- **Definition**: $Q$ is orthogonal if
$$
Q^T = Q^{-1}
$$
- Columns of $Q$ are **orthonormal vectors** (unit length, mutually perpendicular).

> NOTE: Orthogonal matrices preserve lengths and angles.

---

## 6. Permutation Matrices

- **Definition**: A permutation matrix $P$ has **rows of the identity matrix $I$ in any order**.
- There are $n!$ different $n \times n$ permutation matrices.
- **Application**: Permuting components of a vector:
$$
Px = \begin{bmatrix} x_2 \\ x_1 \\ x_3 \\ \dots \end{bmatrix}
$$
- Transpose relation:
$$
P^T = P^{-1}
$$

---

## 7. Connection of Transpose with Inner Products

- Transpose $A^T$ satisfies:
$$
(Ax)^T y = x^T (A^T y) \quad \text{for all vectors } x, y
$$
- **Explanation**:
  - $(Ax)^T$ is a row vector.
  - $x^T A^T$ produces the same combination using rows of $A^T$.
- Ensures formulas for $(AB)^T = B^T A^T$ and $(A^{-1})^T = (A^T)^{-1}$ are consistent.

---

## 8. Examples

1. Let
$$
A = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}, \quad y = \begin{bmatrix} y_1 \\ y_2 \end{bmatrix}
$$
- Compute $(Ax)^T y = x^T (A^T y)$ to verify.

2. Symmetric matrix example:
$$
S = S^T = \begin{bmatrix} 1 & 2 \\ 2 & 3 \end{bmatrix}, \quad S^{-1} = \begin{bmatrix} 3 & -2 \\ -2 & 1 \end{bmatrix}, \quad (S^{-1})^T = S^{-1}
$$

3. Permutation example:
$$
P = \begin{bmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix}, \quad P^T = P^{-1}
$$

---

## 9. Summary Notes

- Transpose flips rows and columns.
- $(AB)^T = B^T A^T$ → reverse order.
- $(A^{-1})^T = (A^T)^{-1}$ → inverse of transpose = transpose of inverse.
- Symmetric matrices: $S^T = S$, inverse symmetric too.
- Orthogonal matrices: $Q^T = Q^{-1}$.
- Permutations: $P^T = P^{-1}$.
- Inner and outer products connect naturally with transpose operations.

> NOTE: Understanding transposes is key to linear algebra, especially for matrix products, solving systems, and understanding symmetry.


# 2.7 Symmetric Products, LDLT, and Permutations

> NOTE: This section discusses **symmetric products**, **LDLT factorization**, and **permutation matrices** in detail.

---

## 1. Symmetric Products: $A^T A$ and $A A^T$

- Let $A$ be any matrix (possibly rectangular).  
- Multiply $A^T A$:
$$
S = A^T A
$$
- **Property**: $S$ is **square and symmetric**:
$$
(A^T A)^T = A^T (A^T)^T = A^T A
$$
- Entry-wise explanation:
$$
(S)_{ij} = \text{dot product of column } i \text{ of } A \text{ with column } j \text{ of } A
$$
- $A A^T$ is also symmetric:
$$
(A A^T)^T = A A^T
$$
- **Dimensions**:
  - $A^T A$ is $n \times n$
  - $A A^T$ is $m \times m$
- **Application**: Least squares and many scientific computations.

> NOTE: Symmetry allows computational savings; only half of the matrix needs to be computed.

---

## 2. Symmetric Factorization: $LDL^T$

- For symmetric $S = S^T$, the $LDU$ factorization simplifies:
$$
S = LDL^T
$$
- **Explanation**:
  - $L$ = lower triangular with 1's on the diagonal
  - $D$ = diagonal matrix containing pivots
  - $U = L^T$ for symmetry
- **Advantage**: Work and storage are roughly halved.
- **Transpose check**:
$$
(L D L^T)^T = L D L^T = S
$$

> NOTE: LDLT factorization preserves symmetry and is widely used for efficient computation.

---

## 3. Permutation Matrices

- **Definition**: A permutation matrix $P$ has exactly **one "1" per row and column**; all other entries are 0.
- **Properties**:
  - $P^T = P^{-1}$
  - Any product of permutation matrices is a permutation matrix.
- **Construction**:
  - Start with identity $I$
  - Apply row exchanges $P_{ij}$
  - Combine multiple exchanges to get any permutation
- Number of permutation matrices of order $n$: $n!$

> NOTE: Each row exchange is its own transpose and its own inverse.

---

## 4. Factorization with Permutations: $PA = LU$

- Sometimes row exchanges are needed to create pivots.
- Factorization:
$$
PA = LU
$$
- $P$ is the permutation matrix encoding all row exchanges.
- Two approaches:
  1. Apply all row exchanges **before elimination** → $PA = LU$
  2. Apply row exchanges **after elimination** → $A = L_1 P_1 U_1$
- **Most common in practice**: $PA = LU$
## 5. Summary Notes

- $A^T A$ and $A A^T$ are symmetric; dimensions differ.
- LDLT factorization preserves symmetry: $S = LDL^T$    
- $P$ encodes row permutations; $P^T = P^{-1}$
- PA = LU includes pivoting in Gaussian elimination.
- Symmetry reduces computation and storage.