This section explores **how vectors relate to each other** in terms of independence, spanning, and basis — and how these ideas lead us to the **dimension** of a vector space.  
We’ll go step by step, with both **intuitive** and **formal** explanations.

---

## 1. Linear Independence and Dependence

### Definition

A sequence of vectors $v_1, v_2, ..., v_n$ is **linearly independent** if the only combination that gives the zero vector is:

$$
x_1v_1 + x_2v_2 + \cdots + x_nv_n = 0
$$

where **all** the coefficients are zero: $x_1 = x_2 = ... = x_n = 0$.

If there exists **any nonzero combination** of $v_1, ..., v_n$ that gives the zero vector,  
then the vectors are **linearly dependent**.

> Intuition:  
> Independence means *none* of the vectors can be “built” or “copied” by combining the others.

---

### Visual Understanding in $\mathbb{R}^3$

- **Independent:**  
  If three vectors are not in the same plane, they point in different directions in space.  
  Only $0v_1 + 0v_2 + 0v_3 = 0$ gives the zero vector.

- **Dependent:**  
  If three vectors lie **in the same plane**, then one can be written as a combination of the other two.  
  For example: $w_1 - w_2 + w_3 = 0$.

---

### Examples in $\mathbb{R}^2$

| Vectors | Independent or Dependent? | Why |
|----------|---------------------------|-----|
| $(1,0)$ and $(0,1)$ | Independent | Standard basis, clearly different directions |
| $(1,0)$ and $(1,0.00001)$ | Independent | Almost parallel, but not exactly |
| $(1,1)$ and $(-1,-1)$ | Dependent | One is a scalar multiple of the other |
| $(1,1)$ and $(0,0)$ | Dependent | The zero vector destroys independence |
| Any 3 vectors in $\mathbb{R}^2$ | Dependent | There are only 2 “directions” possible in $\mathbb{R}^2$ |

---

### Algebraic Test for Independence

To check independence of columns of a matrix $A$:
- Solve the **homogeneous system** $A x = 0$.
- If the **only solution is $x = 0$**, then the columns are **independent**.
- If there is **any nonzero solution**, then they are **dependent**.

> [!note] (Key theorem)
> The columns of $A$ are independent if and only if $N(A) = \{0\}$ (the nullspace contains only the zero vector).

---

### Example

$$
A =
\begin{bmatrix}
1 & 2 & 3 \\
1 & 2 & 3 \\
0 & 0 & 0
\end{bmatrix}
$$

Solving $A x = 0$ gives $x = (-3, 1, 1)$, a **nonzero solution**.  
Therefore, the columns are **dependent**.

Here, the **rank $r = 2 < 3$**, showing that not all columns are pivots.  
Dependent columns mean the rank is **less than the number of columns**.

---

### Full Column Rank and Dependence

If a matrix has **full column rank ($r = n$)**:
- Every column is a pivot column.
- There are **no free variables**.
- $N(A) = \{0\}$.
- The columns are **independent**.

If the matrix is **wide** ($n > m$), then there must be dependence:
- You can’t have more independent vectors than the dimension allows.
- Example: 7 vectors in $\mathbb{R}^5$ must be dependent because $r \leq 5$.

---

### Key Rule
> Any set of $n$ vectors in $\mathbb{R}^m$ is **linearly dependent** if $n > m$.

Reason: there will be at least $(n - m)$ free variables in $A x = 0$ → nonzero solutions → dependence.

---

## 2. Spanning a Subspace

### Definition

A set of vectors **spans** a space if all combinations of those vectors **fill** that space.

In other words, the set $\{v_1, ..., v_n\}$ spans the subspace $S$ if:

$$
S = \{ x_1 v_1 + x_2 v_2 + \cdots + x_n v_n \}
$$

> The **column space** of $A$ is the subspace spanned by its columns.

---

### Examples

1. $v_1 = [1, 0]^T$ and $v_2 = [0, 1]^T$  
   → Span all of $\mathbb{R}^2$

2. $v_1 = [1, 1]^T$, $v_2 = [2, 2]^T$  
   → Span only the line $y = x$

3. $w_1 = [1, 2]^T$ alone  
   → Spans a line in $\mathbb{R}^2$

---

### Column Space and Row Space

- The **column space** $C(A)$ is spanned by the columns of $A$ — it lives in $\mathbb{R}^m$.
- The **row space** of $A$ is spanned by the rows — it lives in $\mathbb{R}^n$.

> The row space of $A$ is the column space of $A^T$:  
> $$
> \text{Row Space}(A) = C(A^T)
> $$

---

## 3. Basis of a Vector Space

### Definition

A **basis** of a vector space is a sequence of vectors that are:
1. **Linearly independent**, and  
2. **Span the space**.

A basis is “just right”:  
- Enough vectors to span the space,  
- No extra ones that would make them dependent.

---

### Consequence

Every vector $v$ in the space can be written **uniquely** as a combination of the basis vectors.

If
$$
v = a_1 v_1 + \cdots + a_n v_n
$$
and also
$$
v = b_1 v_1 + \cdots + b_n v_n
$$
then subtracting gives $(a_1 - b_1)v_1 + \cdots + (a_n - b_n)v_n = 0$.  
By independence, each $(a_i - b_i) = 0$, so $a_i = b_i$.  
→ The representation is **unique**.

---

### Examples

1. **Standard basis for $\mathbb{R}^2$**

$$
i = \begin{bmatrix}1 \\ 0\end{bmatrix}, \quad j = \begin{bmatrix}0 \\ 1\end{bmatrix}
$$

They are independent and span $\mathbb{R}^2$.

2. **Columns of any invertible matrix form a basis for $\mathbb{R}^n$**

If $A$ is invertible, $A x = b$ always has a unique solution →  
Columns are independent and span the whole space.

3. **Pivot columns**  
The pivot columns of a matrix $A$ form a basis for its **column space**.  
Similarly, the **nonzero rows** of the echelon form $R$ form a basis for the **row space**.

---

### Example

$$
A = \begin{bmatrix}
2 & 4 \\
1 & 2
\end{bmatrix} \to R = \begin{bmatrix}
1 & 2 \\
0 & 0
\end{bmatrix}
$$

- **Column space basis:** first column of $A$ → $\begin{bmatrix}2 \\ 1\end{bmatrix}$  
- **Row space basis:** first row of $R$ → $(1, 2)$

Rank $r = 1$, so each space has **dimension 1**.

---

## 4. Dimension of a Vector Space

### Definition

The **dimension** of a space is the number of vectors in any basis of that space.

> All bases for a given vector space contain the **same number** of vectors.

---

### Theorem (Consistency of Dimension)

If $v_1, ..., v_m$ and $w_1, ..., w_n$ are both bases for the same vector space, then $m = n$.

**Proof Idea:**
- Suppose $n > m$.
- Each $w_i$ is a combination of the $v$’s: $W = V A$ (with $A$ being $m \times n$).
- Since $A$ is wide ($n > m$), $A x = 0$ has a **nonzero solution**.
- Then $W x = V (A x) = 0$ → dependence among $w_i$.
- Contradiction, because bases must be independent.
- Therefore, $m = n$.

---

### Examples of Dimensions

| Space | Dimension | Typical Basis |
|--------|------------|----------------|
| Line through origin in $\mathbb{R}^3$ | 1 | Single nonzero vector |
| Plane in $\mathbb{R}^3$ | 2 | Two independent vectors |
| $\mathbb{R}^n$ | n | Standard basis $e_1, ..., e_n$ |
| Nullspace $N(A)$ | $n - r$ | Special solutions of $A x = 0$ |
| Column space $C(A)$ | $r$ | Pivot columns of $A$ |

---

### Important Relation

> [!note] (Rank-Nullity Theorem)
> For any matrix $A$ with $n$ columns and rank $r$:  
> $$
> \text{dim}(C(A)) + \text{dim}(N(A)) = r + (n - r) = n
> $$

---

## 5. Bases for Other Spaces

### Matrix Spaces

Consider the space of **all $2 \times 2$ matrices**, $M_{2 \times 2}$.

Its **dimension** is 4, because each entry can be chosen freely.

One possible basis:

$$
A_1 = \begin{bmatrix}1&0\\0&0\end{bmatrix}, \quad
A_2 = \begin{bmatrix}0&1\\0&0\end{bmatrix}, \quad
A_3 = \begin{bmatrix}0&0\\1&0\end{bmatrix}, \quad
A_4 = \begin{bmatrix}0&0\\0&1\end{bmatrix}
$$

Any $2\times2$ matrix can be written as:
$$
A = e_1 A_1 + e_2 A_2 + e_3 A_3 + e_4 A_4
$$

---

#### Subspaces and Their Dimensions

| Subspace | Example Elements | Dimension |
|-----------|------------------|------------|
| Upper triangular matrices | $\begin{bmatrix}a & b\\0 & c\end{bmatrix}$ | 3 |
| Diagonal matrices | $\begin{bmatrix}a & 0\\0 & b\end{bmatrix}$ | 2 |
| Symmetric matrices | $\begin{bmatrix}a & b\\b & c\end{bmatrix}$ | 3 |
| General $n\times n$ matrices | $\begin{bmatrix}*\end{bmatrix}_{n\times n}$ | $n^2$ |

---

### Function Spaces

Linear algebra also applies to **functions**!

Example differential equations:

| Equation | Solution Space | Basis | Dimension |
|-----------|----------------|--------|------------|
| $y'' = 0$ | all linear functions | $\{1, x\}$ | 2 |
| $y'' = -y$ | sinusoids | $\{\sin x, \cos x\}$ | 2 |
| $y'' = y$ | exponentials | $\{e^x, e^{-x}\}$ | 2 |

> These are **function spaces** where the “vectors” are functions,  
> and linear combinations are formed by addition and scalar multiplication.

---

### The Zero Space

The space $Z = \{0\}$ contains only the zero vector.

- It has **dimension 0**.
- The **empty set** $\{\}$ is its basis (no vectors needed).
- You **cannot** include the zero vector in a basis, because that destroys independence.

---

# Summary Table

| Concept | Definition | Test / Example |
|----------|-------------|----------------|
| **Independence** | Only the zero combination gives 0 | $A x = 0$ has only $x = 0$ |
| **Dependence** | Some nonzero combination gives 0 | $A x = 0$ has nonzero $x$ |
| **Span** | All combinations fill the space | Columns span column space |
| **Basis** | Independent + spanning | Pivot columns or rows |
| **Dimension** | # of vectors in any basis | Rank of $A$ for $C(A)$, $n - r$ for $N(A)$ |

---

> [!summary]
> - Independence means *no redundancy*.  
> - Span means *coverage*.  
> - A basis achieves both: *enough, but not too many*.  
> - Dimension counts the number of *directions of freedom*.  
> - Rank measures how many columns (or rows) are independent.  
> - Every space has many bases — but always the same dimension.